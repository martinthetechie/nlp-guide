{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOxRBQEz9W3aBdYezXlZT9Q",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/martinthetechie/nlp-guide/blob/main/nlp_guide_tokenizers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## <h2> Tokenizers </h2>\n",
        "\n",
        "- [Word Tokenizer](#word-tokenizer)\n",
        "- [Subword Tokenizer](#subword-tokenizer)\n",
        "- [Character Tokenizer](#character-tokenizer)"
      ],
      "metadata": {
        "id": "dBzZydx5Ux-O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Word Tokenizer"
      ],
      "metadata": {
        "id": "LHA23JCqVu0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = 'Tokenization is essential in NLP'"
      ],
      "metadata": {
        "id": "cLcp3ojPWv7t"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "doc= nlp(text)\n",
        "tokens = [token.text for token in doc]\n",
        "print(\"Spacy Word Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IObHgbLPVtiY",
        "outputId": "58a7780e-52f6-45a0-e175-dfd35c7d2c18"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spacy Word Tokens: ['Tokenization', 'is', 'essential', 'in', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "tokens = word_tokenize(text)\n",
        "print(\"NLTK Word Tokens:\", tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f7Hfy7mZV5GX",
        "outputId": "b18d02ac-e965-47ee-ab40-3077dbfecf23"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Word Tokens: ['Tokenization', 'is', 'essential', 'in', 'NLP']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Subword Tokenizer"
      ],
      "metadata": {
        "id": "dX2MKVylXF_v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "subwords = tokenize.tokenize(text)\n",
        "print(\"Bert Subword Tokens:\", subwords)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1CGSnSBXHnV",
        "outputId": "e6365a44-64c4-4376-a8af-eb19e63992cb"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bert Subword Tokens: ['token', '##ization', 'is', 'essential', 'in', 'nl', '##p']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_text = tokenizer.convert_tokens_to_string(subwords)\n",
        "print(\"Decoded Text:\", decoded_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "de62Zu2sYOvy",
        "outputId": "b32f5486-282b-4113-d886-a40dafbaab45"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Decoded Text: tokenization is essential in nlp\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Character Tokenizer"
      ],
      "metadata": {
        "id": "Bh_bJ7MRYkug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_word = 'NLP'\n",
        "characters = list(text_word)\n",
        "print(\"Character Tokens:\", characters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NI_irsrfYpob",
        "outputId": "99dee3d0-1cb1-42f5-a745-75c4822bea99"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Character Tokens: ['N', 'L', 'P']\n"
          ]
        }
      ]
    }
  ]
}